{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Oza5_m5D3_o"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import scipy.spatial.distance\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import sys\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Ply**"
      ],
      "metadata": {
        "id": "XNeW5xNE88qa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urQh8KdlHD9I"
      },
      "outputs": [],
      "source": [
        "ply_dtypes = dict([\n",
        "    (b'int8', 'i1'),\n",
        "    (b'char', 'i1'),\n",
        "    (b'uint8', 'u1'),\n",
        "    (b'uchar', 'b1'),\n",
        "    (b'uchar', 'u1'),\n",
        "    (b'int16', 'i2'),\n",
        "    (b'short', 'i2'),\n",
        "    (b'uint16', 'u2'),\n",
        "    (b'ushort', 'u2'),\n",
        "    (b'int32', 'i4'),\n",
        "    (b'int', 'i4'),\n",
        "    (b'uint32', 'u4'),\n",
        "    (b'uint', 'u4'),\n",
        "    (b'float32', 'f4'),\n",
        "    (b'float', 'f4'),\n",
        "    (b'float64', 'f8'),\n",
        "    (b'double', 'f8')\n",
        "])\n",
        "\n",
        "valid_formats = {'ascii': '', 'binary_big_endian': '>',\n",
        "                 'binary_little_endian': '<'}\n",
        "\n",
        "def parse_header(plyfile, ext):\n",
        "    line = []\n",
        "    properties = []\n",
        "    num_points = None\n",
        "\n",
        "    while b'end_header' not in line and line != b'':\n",
        "        line = plyfile.readline()\n",
        "        if b'element' in line:\n",
        "            line = line.split()\n",
        "            num_points = int(line[2])\n",
        "\n",
        "        elif b'property' in line:\n",
        "            line = line.split()\n",
        "            properties.append((line[2].decode(), ext + ply_dtypes[line[1]]))\n",
        "\n",
        "    return num_points, properties\n",
        "\n",
        "def read_ply(filename):\n",
        "    with open(filename, 'rb') as plyfile:\n",
        "        if b'ply' not in plyfile.readline():\n",
        "            raise ValueError('The file does not start whith the word ply')\n",
        "\n",
        "        fmt = plyfile.readline().split()[1].decode()\n",
        "        if fmt == \"ascii\":\n",
        "            raise ValueError('The file is not binary')\n",
        "\n",
        "        ext = valid_formats[fmt]\n",
        "        num_points, properties = parse_header(plyfile, ext)\n",
        "        data = np.fromfile(plyfile, dtype=properties, count=num_points)\n",
        "    return data\n",
        "\n",
        "\n",
        "def header_properties(field_list, field_names):\n",
        "    lines = []\n",
        "    lines.append('element vertex %d' % field_list[0].shape[0])\n",
        "\n",
        "    i = 0\n",
        "    for fields in field_list:\n",
        "        for field in fields.T:\n",
        "            lines.append('property %s %s' % (field.dtype.name, field_names[i]))\n",
        "            i += 1\n",
        "\n",
        "    return lines\n",
        "\n",
        "def write_ply(filename, field_list, field_names):\n",
        "\n",
        "    field_list = list(field_list) if (type(field_list) == list or type(field_list) == tuple) else list((field_list,))\n",
        "    for i, field in enumerate(field_list):\n",
        "        if field is None:\n",
        "            print('WRITE_PLY ERROR: a field is None')\n",
        "            return False\n",
        "        elif field.ndim > 2:\n",
        "            print('WRITE_PLY ERROR: a field have more than 2 dimensions')\n",
        "            return False\n",
        "        elif field.ndim < 2:\n",
        "            field_list[i] = field.reshape(-1, 1)\n",
        "\n",
        "    n_points = [field.shape[0] for field in field_list]\n",
        "    if not np.all(np.equal(n_points, n_points[0])):\n",
        "        print('wrong field dimensions')\n",
        "        return False\n",
        "\n",
        "    n_fields = np.sum([field.shape[1] for field in field_list])\n",
        "    if (n_fields != len(field_names)):\n",
        "        print('wrong number of field names')\n",
        "        return False\n",
        "\n",
        "    if not filename.endswith('.ply'):\n",
        "        filename += '.ply'\n",
        "\n",
        "    with open(filename, 'w') as plyfile:\n",
        "\n",
        "        header = ['ply']\n",
        "\n",
        "        header.append('format binary_' + sys.byteorder + '_endian 1.0')\n",
        "\n",
        "        header.extend(header_properties(field_list, field_names))\n",
        "\n",
        "        header.append('end_header')\n",
        "\n",
        "        for line in header:\n",
        "            plyfile.write(\"%s\\n\" % line)\n",
        "\n",
        "    with open(filename, 'ab') as plyfile:\n",
        "\n",
        "        i = 0\n",
        "        type_list = []\n",
        "        for fields in field_list:\n",
        "            for field in fields.T:\n",
        "                type_list += [(field_names[i], field.dtype.str)]\n",
        "                i += 1\n",
        "        data = np.empty(field_list[0].shape[0], dtype=type_list)\n",
        "        i = 0\n",
        "        for fields in field_list:\n",
        "            for field in fields.T:\n",
        "                data[field_names[i]] = field\n",
        "                i += 1\n",
        "\n",
        "        data.tofile(plyfile)\n",
        "\n",
        "    return True\n",
        "\n",
        "def describe_element(name, df):\n",
        "\n",
        "    property_formats = {'f': 'float', 'u': 'uchar', 'i': 'int'}\n",
        "    element = ['element ' + name + ' ' + str(len(df))]\n",
        "\n",
        "    if name == 'face':\n",
        "        element.append(\"property list uchar int points_indices\")\n",
        "\n",
        "    else:\n",
        "        for i in range(len(df.columns)):\n",
        "            f = property_formats[str(df.dtypes[i])[0]]\n",
        "            element.append('property ' + f + ' ' + df.columns.values[i])\n",
        "\n",
        "    return element\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Classes pour augmentation de donnees"
      ],
      "metadata": {
        "id": "R4rSwK6v9Di4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mATlCpIq1vPg"
      },
      "outputs": [],
      "source": [
        "class RandomRotation_z(object):\n",
        "    def __call__(self, pointcloud):\n",
        "        theta = random.random() * 2. * math.pi\n",
        "        rot_matrix = np.array([[math.cos(theta), -math.sin(theta),      0],\n",
        "                               [math.sin(theta),  math.cos(theta),      0],\n",
        "                               [0,                               0,      1]])\n",
        "        rot_pointcloud = rot_matrix.dot(pointcloud.T).T\n",
        "        return rot_pointcloud\n",
        "\n",
        "class RandomScale(object):\n",
        "    def __init__(self):\n",
        "        mini = random.random()\n",
        "        maxi = 1+ random.random()\n",
        "        self.scale = maxi - mini\n",
        "        self.bias = mini\n",
        "    def __call__(self, coords):\n",
        "        s = self.scale * np.random.rand(1) + self.bias\n",
        "        return coords * s\n",
        "\n",
        "class RandomTranslation(object):\n",
        "    def __call__(self, coords):\n",
        "        trans = 0.05 * np.random.randn(1, 3)\n",
        "        return coords + trans\n",
        "\n",
        "class RandomNoise(object):\n",
        "    def __call__(self, pointcloud):\n",
        "        noise = np.random.normal(0, 0.02, (pointcloud.shape))\n",
        "        noisy_pointcloud = pointcloud + noise\n",
        "        return noisy_pointcloud\n",
        "\n",
        "class ShufflePoints(object):\n",
        "    def __call__(self, pointcloud):\n",
        "        np.random.shuffle(pointcloud)\n",
        "        return pointcloud\n",
        "\n",
        "class RandomFlip(object):\n",
        "    def __call__(self, pointcloud):\n",
        "        if np.random.rand() > 0.5:\n",
        "            pointcloud[:, 0] = -pointcloud[:, 0]  # Flip X-axis\n",
        "        if np.random.rand() > 0.5:\n",
        "            pointcloud[:, 1] = -pointcloud[:, 1]  # Flip Y-axis\n",
        "        return pointcloud\n",
        "'''\n",
        "class RandomDropout(object):\n",
        "    def __call__(self, pointcloud, dropout_rate=0.2):\n",
        "        num_points = pointcloud.shape[0]\n",
        "        mask = np.random.rand(num_points) > dropout_rate\n",
        "        return pointcloud[mask]\n",
        "\n",
        "class RandomShear(object):\n",
        "    def __call__(self, pointcloud):\n",
        "        shear_matrix = np.array([[1, np.random.uniform(-0.2, 0.2), 0],\n",
        "                                 [np.random.uniform(-0.2, 0.2), 1, 0],\n",
        "                                 [0, 0, 1]])\n",
        "        return pointcloud @ shear_matrix.T\n",
        "'''\n",
        "\n",
        "\n",
        "class ToTensor(object):\n",
        "    def __call__(self, pointcloud):\n",
        "        return torch.from_numpy(pointcloud)\n",
        "\n",
        "\n",
        "def default_transforms():\n",
        "    return transforms.Compose([\n",
        "        RandomRotation_z(),\n",
        "        RandomNoise(),\n",
        "        ShufflePoints(),\n",
        "        RandomScale(),\n",
        "        RandomTranslation(),\n",
        "        RandomFlip(),\n",
        "        ToTensor()\n",
        "    ])\n",
        "\n",
        "\n",
        "class PointCloudData(Dataset):\n",
        "    def __init__(self, root_dir, folder=\"train\", transform=default_transforms()):\n",
        "        self.root_dir = root_dir\n",
        "        folders = [dir for dir in sorted(os.listdir(root_dir)) if os.path.isdir(root_dir+\"/\"+dir)]\n",
        "        self.classes = {folder: i for i, folder in enumerate(folders)}\n",
        "        self.transforms = transform\n",
        "        self.files = []\n",
        "        for category in self.classes.keys():\n",
        "            new_dir = root_dir+\"/\"+category+\"/\"+folder\n",
        "            for file in os.listdir(new_dir):\n",
        "                if file.endswith('.ply'):\n",
        "                    sample = {}\n",
        "                    sample['ply_path'] = new_dir+\"/\"+file\n",
        "                    sample['category'] = category\n",
        "                    self.files.append(sample)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ply_path = self.files[idx]['ply_path']\n",
        "        category = self.files[idx]['category']\n",
        "        data = read_ply(ply_path)\n",
        "        pointcloud = self.transforms(np.vstack((data['x'], data['y'], data['z'])).T)\n",
        "        return {'pointcloud': pointcloud, 'category': self.classes[category]}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PointMLP**"
      ],
      "metadata": {
        "id": "IgtvyVQg9Tai"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-qjvvtrE8eH"
      },
      "outputs": [],
      "source": [
        "class PointMLP(nn.Module):\n",
        "    def __init__(self, num_classes=40):\n",
        "        super(PointMLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(3072, 512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.fc3 = nn.Linear(256, num_classes)\n",
        "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.reshape(-1, 3072)  # équivalent à un flatten\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        x = self.log_softmax(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PointNet Basic**"
      ],
      "metadata": {
        "id": "5ZpN-TIx9buJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V25-TBMvvMf1"
      },
      "outputs": [],
      "source": [
        "class PointNetBasic(nn.Module):\n",
        "    def __init__(self, num_classes=40):\n",
        "        super(PointNetBasic, self).__init__()\n",
        "        # Première convolution (entrée de 3, sortie de 64)\n",
        "        self.conv1 = nn.Conv1d(3, 64, 1)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        # Deuxième convolution\n",
        "        self.conv2 = nn.Conv1d(64, 64, 1)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        # Troisième convolution\n",
        "        self.conv3 = nn.Conv1d(64, 64, 1)\n",
        "        self.bn3 = nn.BatchNorm1d(64)\n",
        "        # Quatrième convolution\n",
        "        self.conv4 = nn.Conv1d(64, 128, 1)\n",
        "        self.bn4 = nn.BatchNorm1d(128)\n",
        "        # Cinquième convolution (entrée de 128, sortie de 1024)\n",
        "        self.conv5 = nn.Conv1d(128, 1024, 1)\n",
        "        self.bn5 = nn.BatchNorm1d(1024)\n",
        "\n",
        "        self.maxpool = nn.MaxPool1d(1024) # Max pooling\n",
        "\n",
        "        # Première couche fully connected (entrée de 1024, sortie de 512)\n",
        "        self.fc1 = nn.Linear(1024, 512)\n",
        "        self.bn6 = nn.BatchNorm1d(512)\n",
        "        # Deuxième couche\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.bn7 = nn.BatchNorm1d(256)\n",
        "        # Troisième couche\n",
        "        self.fc3 = nn.Linear(256, num_classes)\n",
        "        # Activation LogSoftmax\n",
        "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Passage à travers les convo et les batch normalizations avec ReLU\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = F.relu(self.bn5(self.conv5(x)))\n",
        "\n",
        "        # Max pooling\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        # Redimensionnement de l'entrée pour correspondre à la première couche fully connected\n",
        "        x = x.reshape(-1, 1024)\n",
        "\n",
        "        # Passage à travers les couches fully connected avec batch normalization et dropout\n",
        "        x = F.relu(self.bn6(self.fc1(x)))\n",
        "        x = F.relu(self.bn7(self.dropout(self.fc2(x))))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return self.log_softmax(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tnet**"
      ],
      "metadata": {
        "id": "xcfD-Sii9gdi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IF_Yv271DvJM"
      },
      "outputs": [],
      "source": [
        "class Tnet(nn.Module):\n",
        "    def __init__(self, k=3):\n",
        "        super(Tnet, self).__init__()\n",
        "        # Première convolution (entrée de 3, sortie de 64)\n",
        "        self.conv1 = nn.Conv1d(3, 64, 1)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        # Deuxième convolution\n",
        "        self.conv2 = nn.Conv1d(64, 128, 1)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        # Troisième convolution (entrée de 128, sortie de 1024)\n",
        "        self.conv3 = nn.Conv1d(128, 1024, 1)\n",
        "        self.bn3 = nn.BatchNorm1d(1024)\n",
        "\n",
        "        # Max pooling\n",
        "        self.maxpool = nn.MaxPool1d(1024)\n",
        "\n",
        "        # Première couche fully connected (entrée de 1024, sortie de 512)\n",
        "        self.fc1 = nn.Linear(1024, 512)\n",
        "        self.bn4 = nn.BatchNorm1d(512)\n",
        "        # Deuxième couche\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.bn5 = nn.BatchNorm1d(256)\n",
        "        # Troisième couche fully connected (entrée de 256, sortie de k*k)\n",
        "        self.fc3 = nn.Linear(256, k * k)\n",
        "\n",
        "        self.k = k\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Passage à travers les convo et les batch normalizations avec activation ReLU\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "\n",
        "        # Max pooling\n",
        "        x = self.maxpool(x)\n",
        "        # Redimensionnement de l'entrée pour correspondre à la première couche fully connected\n",
        "        x = x.reshape(-1, 1024)\n",
        "\n",
        "        # Passage à travers les couches fully connected avec batch normalization\n",
        "        x = F.relu(self.bn4(self.fc1(x)))\n",
        "        x = F.relu(self.bn5(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        # Ajout de l'identité pour former une matrice 3x3\n",
        "        id3x3 = torch.eye(self.k, requires_grad=True).repeat(x.shape[0], 1, 1)\n",
        "        if x.is_cuda:\n",
        "            id3x3 = id3x3.cuda()\n",
        "        x = x.view(-1, self.k, self.k)\n",
        "        x = x + id3x3\n",
        "        x = x.view(-1, self.k, self.k)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PointNet Full**"
      ],
      "metadata": {
        "id": "6yoMc-mJ9kux"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtUvFTXht2Lo"
      },
      "outputs": [],
      "source": [
        "class PointNetFull(nn.Module):\n",
        "    def __init__(self, num_classes=40):\n",
        "        super(PointNetFull, self).__init__()\n",
        "        self.tnet = Tnet(k=3)\n",
        "\n",
        "        # Première convolution (entrée de 3, sortie de 64)\n",
        "        self.conv1 = nn.Conv1d(3, 64, 1)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        # Deuxième convolution\n",
        "        self.conv2 = nn.Conv1d(64, 64, 1)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "\n",
        "        # Troisième convolution\n",
        "        self.conv3 = nn.Conv1d(64, 64, 1)\n",
        "        self.bn3 = nn.BatchNorm1d(64)\n",
        "        # Quatrième convolution\n",
        "        self.conv4 = nn.Conv1d(64, 128, 1)\n",
        "        self.bn4 = nn.BatchNorm1d(128)\n",
        "        # Cinquième convolution (entrée de 128, sortie de 1024)\n",
        "        self.conv5 = nn.Conv1d(128, 1024, 1)\n",
        "        self.bn5 = nn.BatchNorm1d(1024)\n",
        "\n",
        "        # Max pooling\n",
        "        self.maxpool = nn.MaxPool1d(1024)\n",
        "\n",
        "        # Première couche fully connected (entrée de 1024, sortie de 512)\n",
        "        self.fc1 = nn.Linear(1024, 512)\n",
        "        self.bn6 = nn.BatchNorm1d(512)\n",
        "        # Deuxième couche\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.bn7 = nn.BatchNorm1d(256)\n",
        "        # Troisième couche fully connected (entrée de 256, sortie du nombre de classes)\n",
        "        self.fc3 = nn.Linear(256, num_classes)\n",
        "\n",
        "        # Activation LogSoftmax\n",
        "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Application du Tnet pour obtenir la transformation\n",
        "        tnet_output = self.tnet(x)\n",
        "\n",
        "        # Multiplication matricielle de l'entrée avec la transformation obtenue\n",
        "        x = torch.matmul(tnet_output, x)\n",
        "\n",
        "        # Passage à travers les convo et les batch normalizations avec ReLU\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = F.relu(self.bn5(self.conv5(x)))\n",
        "\n",
        "        # Max pooling\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        # Redimensionnement de l'entrée pour correspondre à la première couche fully connected\n",
        "        x = x.reshape(-1, 1024)\n",
        "\n",
        "        # Passage à travers les couches fully connected avec batch normalization et dropout\n",
        "        x = F.relu(self.bn6(self.fc1(x)))\n",
        "        x = F.relu(self.bn7(self.dropout(self.fc2(x))))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return self.log_softmax(x), tnet_output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fonctions"
      ],
      "metadata": {
        "id": "qFnKb_QJ9r8x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzMBk3AmGtwK"
      },
      "outputs": [],
      "source": [
        "def basic_loss(outputs, labels):\n",
        "    criterion = torch.nn.NLLLoss()\n",
        "    bs=outputs.size(0)\n",
        "    return criterion(outputs, labels)\n",
        "\n",
        "def pointnet_full_loss(outputs, labels, m3x3, alpha = 0.001):\n",
        "    criterion = torch.nn.NLLLoss()\n",
        "    bs=outputs.size(0)\n",
        "    id3x3 = torch.eye(3, requires_grad=True).repeat(bs,1,1)\n",
        "    if outputs.is_cuda:\n",
        "        id3x3=id3x3.cuda()\n",
        "    diff3x3 = id3x3-torch.bmm(m3x3,m3x3.transpose(1,2))\n",
        "    return criterion(outputs, labels) + alpha * (torch.norm(diff3x3)) / float(bs)\n",
        "\n",
        "\n",
        "def train(model, device, train_loader, test_loader=None, epochs=250):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "    loss=0\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs, labels = data['pointcloud'].to(device).float(), data['category'].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            # outputs = model(inputs.transpose(1,2))\n",
        "            outputs, m3x3 = model(inputs.transpose(1,2))\n",
        "            # loss = basic_loss(outputs, labels)\n",
        "            loss = pointnet_full_loss(outputs, labels, m3x3)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        correct = total = 0\n",
        "        if test_loader:\n",
        "            with torch.no_grad():\n",
        "                for data in test_loader:\n",
        "                    inputs, labels = data['pointcloud'].to(device).float(), data['category'].to(device)\n",
        "                    # outputs = model(inputs.transpose(1,2))\n",
        "                    outputs, __ = model(inputs.transpose(1,2))\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()\n",
        "            val_acc = 100. * correct / total\n",
        "            print('Epoch: %d, Loss: %.3f, Test accuracy: %.1f %%' %(epoch+1, loss, val_acc))\n",
        "\n",
        "        scheduler.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzHZ7iry-JEg"
      },
      "source": [
        "### **DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCOSfHR2JXKj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "zip_path = \"/content/ModelNet40_PLY.zip\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "  zip_ref.extractall(\"/content/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7qL9MToKgl4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea7b1d84-13a9-4405-dda9-42f3f881bb00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes:  {0: 'airplane', 1: 'bathtub', 2: 'bed', 3: 'bench', 4: 'bookshelf', 5: 'bottle', 6: 'bowl', 7: 'car', 8: 'chair', 9: 'cone', 10: 'cup', 11: 'curtain', 12: 'desk', 13: 'door', 14: 'dresser', 15: 'flower_pot', 16: 'glass_box', 17: 'guitar', 18: 'keyboard', 19: 'lamp', 20: 'laptop', 21: 'mantel', 22: 'monitor', 23: 'night_stand', 24: 'person', 25: 'piano', 26: 'plant', 27: 'radio', 28: 'range_hood', 29: 'sink', 30: 'sofa', 31: 'stairs', 32: 'stool', 33: 'table', 34: 'tent', 35: 'toilet', 36: 'tv_stand', 37: 'vase', 38: 'wardrobe', 39: 'xbox'}\n",
            "Train dataset size:  9843\n",
            "Test dataset size:  2468\n",
            "Number of classes:  40\n",
            "Sample pointcloud shape:  torch.Size([1024, 3])\n"
          ]
        }
      ],
      "source": [
        "t0 = time.time()\n",
        "train_ds = PointCloudData(\"/content/ModelNet40_PLY\")\n",
        "test_ds = PointCloudData(\"/content//ModelNet40_PLY\", folder='test')\n",
        "inv_classes = {i: cat for cat, i in train_ds.classes.items()}\n",
        "print(\"Classes: \", inv_classes)\n",
        "print('Train dataset size: ', len(train_ds))\n",
        "print('Test dataset size: ', len(test_ds))\n",
        "print('Number of classes: ', len(train_ds.classes))\n",
        "print('Sample pointcloud shape: ', train_ds[0]['pointcloud'].size())\n",
        "train_loader = DataLoader(dataset=train_ds, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_ds, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re3riU0G7FPn"
      },
      "source": [
        "### **Test PointNet Full avec 250 epochs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dkFygxq7RFU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a960d65-dd49-4243-9e0e-1a26e1e3b266"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters in the Neural Networks:  1622705\n",
            "Device:  cuda:0\n",
            "Epoch: 1, Loss: 1.687, Test accuracy: 52.1 %\n",
            "Epoch: 2, Loss: 1.471, Test accuracy: 57.4 %\n",
            "Epoch: 3, Loss: 1.834, Test accuracy: 63.9 %\n",
            "Epoch: 4, Loss: 1.030, Test accuracy: 60.9 %\n",
            "Epoch: 5, Loss: 0.662, Test accuracy: 70.4 %\n",
            "Epoch: 6, Loss: 0.885, Test accuracy: 73.8 %\n",
            "Epoch: 7, Loss: 0.776, Test accuracy: 70.8 %\n",
            "Epoch: 8, Loss: 0.978, Test accuracy: 71.1 %\n",
            "Epoch: 9, Loss: 0.799, Test accuracy: 75.2 %\n",
            "Epoch: 10, Loss: 0.660, Test accuracy: 75.4 %\n",
            "Epoch: 11, Loss: 0.805, Test accuracy: 75.9 %\n",
            "Epoch: 12, Loss: 0.777, Test accuracy: 76.1 %\n",
            "Epoch: 13, Loss: 0.673, Test accuracy: 78.5 %\n",
            "Epoch: 14, Loss: 0.725, Test accuracy: 77.4 %\n",
            "Epoch: 15, Loss: 0.848, Test accuracy: 77.4 %\n",
            "Epoch: 16, Loss: 0.689, Test accuracy: 76.8 %\n",
            "Epoch: 17, Loss: 0.401, Test accuracy: 71.9 %\n",
            "Epoch: 18, Loss: 0.697, Test accuracy: 77.6 %\n",
            "Epoch: 19, Loss: 0.960, Test accuracy: 80.0 %\n",
            "Epoch: 20, Loss: 0.598, Test accuracy: 79.6 %\n",
            "Epoch: 21, Loss: 0.253, Test accuracy: 82.4 %\n",
            "Epoch: 22, Loss: 0.323, Test accuracy: 82.8 %\n",
            "Epoch: 23, Loss: 0.697, Test accuracy: 82.2 %\n",
            "Epoch: 24, Loss: 0.511, Test accuracy: 83.7 %\n",
            "Epoch: 25, Loss: 0.470, Test accuracy: 83.3 %\n",
            "Epoch: 26, Loss: 0.401, Test accuracy: 82.6 %\n",
            "Epoch: 27, Loss: 0.497, Test accuracy: 83.1 %\n",
            "Epoch: 28, Loss: 0.421, Test accuracy: 84.0 %\n",
            "Epoch: 29, Loss: 0.665, Test accuracy: 82.9 %\n",
            "Epoch: 30, Loss: 0.371, Test accuracy: 82.9 %\n",
            "Epoch: 31, Loss: 0.412, Test accuracy: 83.8 %\n",
            "Epoch: 32, Loss: 0.432, Test accuracy: 83.3 %\n",
            "Epoch: 33, Loss: 0.530, Test accuracy: 84.6 %\n",
            "Epoch: 34, Loss: 0.787, Test accuracy: 83.5 %\n",
            "Epoch: 35, Loss: 0.895, Test accuracy: 82.1 %\n",
            "Epoch: 36, Loss: 0.254, Test accuracy: 83.6 %\n",
            "Epoch: 37, Loss: 0.380, Test accuracy: 81.5 %\n",
            "Epoch: 38, Loss: 0.477, Test accuracy: 82.7 %\n",
            "Epoch: 39, Loss: 0.394, Test accuracy: 83.6 %\n",
            "Epoch: 40, Loss: 0.209, Test accuracy: 83.4 %\n",
            "Epoch: 41, Loss: 0.724, Test accuracy: 84.8 %\n",
            "Epoch: 42, Loss: 0.226, Test accuracy: 84.5 %\n",
            "Epoch: 43, Loss: 0.496, Test accuracy: 86.0 %\n",
            "Epoch: 44, Loss: 0.302, Test accuracy: 84.2 %\n",
            "Epoch: 45, Loss: 0.277, Test accuracy: 83.8 %\n",
            "Epoch: 46, Loss: 0.737, Test accuracy: 84.8 %\n",
            "Epoch: 47, Loss: 0.312, Test accuracy: 84.6 %\n",
            "Epoch: 48, Loss: 0.624, Test accuracy: 84.4 %\n",
            "Epoch: 49, Loss: 0.177, Test accuracy: 85.3 %\n",
            "Epoch: 50, Loss: 0.436, Test accuracy: 85.7 %\n",
            "Epoch: 51, Loss: 0.629, Test accuracy: 85.1 %\n",
            "Epoch: 52, Loss: 0.665, Test accuracy: 85.4 %\n",
            "Epoch: 53, Loss: 0.671, Test accuracy: 85.7 %\n",
            "Epoch: 54, Loss: 0.392, Test accuracy: 85.3 %\n",
            "Epoch: 55, Loss: 0.421, Test accuracy: 85.3 %\n",
            "Epoch: 56, Loss: 0.172, Test accuracy: 84.2 %\n",
            "Epoch: 57, Loss: 0.172, Test accuracy: 84.6 %\n",
            "Epoch: 58, Loss: 0.143, Test accuracy: 85.6 %\n",
            "Epoch: 59, Loss: 0.129, Test accuracy: 84.8 %\n",
            "Epoch: 60, Loss: 0.111, Test accuracy: 85.5 %\n",
            "Epoch: 61, Loss: 0.501, Test accuracy: 86.0 %\n",
            "Epoch: 62, Loss: 0.403, Test accuracy: 86.0 %\n",
            "Epoch: 63, Loss: 0.137, Test accuracy: 85.6 %\n",
            "Epoch: 64, Loss: 0.309, Test accuracy: 85.9 %\n",
            "Epoch: 65, Loss: 0.160, Test accuracy: 85.9 %\n",
            "Epoch: 66, Loss: 0.458, Test accuracy: 86.7 %\n",
            "Epoch: 67, Loss: 0.176, Test accuracy: 86.0 %\n",
            "Epoch: 68, Loss: 0.175, Test accuracy: 86.6 %\n",
            "Epoch: 69, Loss: 0.312, Test accuracy: 85.2 %\n",
            "Epoch: 70, Loss: 0.412, Test accuracy: 86.8 %\n",
            "Epoch: 71, Loss: 0.493, Test accuracy: 86.3 %\n",
            "Epoch: 72, Loss: 0.229, Test accuracy: 85.9 %\n",
            "Epoch: 73, Loss: 0.040, Test accuracy: 85.8 %\n",
            "Epoch: 74, Loss: 0.428, Test accuracy: 85.8 %\n",
            "Epoch: 75, Loss: 0.236, Test accuracy: 85.3 %\n",
            "Epoch: 76, Loss: 0.447, Test accuracy: 84.6 %\n",
            "Epoch: 77, Loss: 0.327, Test accuracy: 87.1 %\n",
            "Epoch: 78, Loss: 0.306, Test accuracy: 85.0 %\n",
            "Epoch: 79, Loss: 0.135, Test accuracy: 86.4 %\n",
            "Epoch: 80, Loss: 0.405, Test accuracy: 86.5 %\n",
            "Epoch: 81, Loss: 0.236, Test accuracy: 86.6 %\n",
            "Epoch: 82, Loss: 0.241, Test accuracy: 86.4 %\n",
            "Epoch: 83, Loss: 0.183, Test accuracy: 86.2 %\n",
            "Epoch: 84, Loss: 0.141, Test accuracy: 87.4 %\n",
            "Epoch: 85, Loss: 0.081, Test accuracy: 86.5 %\n",
            "Epoch: 86, Loss: 0.078, Test accuracy: 87.3 %\n",
            "Epoch: 87, Loss: 0.233, Test accuracy: 86.1 %\n",
            "Epoch: 88, Loss: 0.715, Test accuracy: 86.1 %\n",
            "Epoch: 89, Loss: 0.188, Test accuracy: 86.3 %\n",
            "Epoch: 90, Loss: 0.367, Test accuracy: 86.8 %\n",
            "Epoch: 91, Loss: 0.123, Test accuracy: 86.5 %\n",
            "Epoch: 92, Loss: 0.400, Test accuracy: 87.1 %\n",
            "Epoch: 93, Loss: 0.269, Test accuracy: 86.6 %\n",
            "Epoch: 94, Loss: 0.316, Test accuracy: 86.9 %\n",
            "Epoch: 95, Loss: 0.350, Test accuracy: 87.0 %\n",
            "Epoch: 96, Loss: 0.310, Test accuracy: 87.0 %\n",
            "Epoch: 97, Loss: 0.249, Test accuracy: 86.0 %\n",
            "Epoch: 98, Loss: 0.131, Test accuracy: 86.5 %\n",
            "Epoch: 99, Loss: 0.052, Test accuracy: 86.7 %\n",
            "Epoch: 100, Loss: 0.290, Test accuracy: 86.3 %\n",
            "Epoch: 101, Loss: 0.448, Test accuracy: 86.6 %\n",
            "Epoch: 102, Loss: 0.232, Test accuracy: 86.3 %\n",
            "Epoch: 103, Loss: 0.454, Test accuracy: 86.6 %\n",
            "Epoch: 104, Loss: 0.114, Test accuracy: 86.5 %\n",
            "Epoch: 105, Loss: 0.123, Test accuracy: 86.1 %\n",
            "Epoch: 106, Loss: 0.129, Test accuracy: 86.8 %\n",
            "Epoch: 107, Loss: 0.109, Test accuracy: 87.0 %\n",
            "Epoch: 108, Loss: 0.143, Test accuracy: 86.5 %\n",
            "Epoch: 109, Loss: 0.406, Test accuracy: 86.5 %\n",
            "Epoch: 110, Loss: 0.136, Test accuracy: 86.6 %\n",
            "Epoch: 111, Loss: 0.532, Test accuracy: 87.0 %\n",
            "Epoch: 112, Loss: 0.318, Test accuracy: 85.9 %\n",
            "Epoch: 113, Loss: 0.215, Test accuracy: 86.3 %\n",
            "Epoch: 114, Loss: 0.137, Test accuracy: 86.6 %\n",
            "Epoch: 115, Loss: 0.316, Test accuracy: 87.2 %\n",
            "Epoch: 116, Loss: 0.440, Test accuracy: 86.5 %\n",
            "Epoch: 117, Loss: 0.211, Test accuracy: 87.1 %\n",
            "Epoch: 118, Loss: 0.308, Test accuracy: 86.4 %\n",
            "Epoch: 119, Loss: 0.204, Test accuracy: 86.4 %\n",
            "Epoch: 120, Loss: 0.405, Test accuracy: 86.7 %\n",
            "Epoch: 121, Loss: 0.272, Test accuracy: 87.5 %\n",
            "Epoch: 122, Loss: 0.121, Test accuracy: 87.0 %\n",
            "Epoch: 123, Loss: 0.633, Test accuracy: 86.8 %\n",
            "Epoch: 124, Loss: 0.313, Test accuracy: 86.7 %\n",
            "Epoch: 125, Loss: 0.325, Test accuracy: 86.6 %\n",
            "Epoch: 126, Loss: 0.077, Test accuracy: 86.8 %\n",
            "Epoch: 127, Loss: 0.110, Test accuracy: 87.1 %\n",
            "Epoch: 128, Loss: 0.306, Test accuracy: 87.0 %\n",
            "Epoch: 129, Loss: 0.366, Test accuracy: 86.6 %\n",
            "Epoch: 130, Loss: 0.288, Test accuracy: 86.5 %\n",
            "Epoch: 131, Loss: 0.279, Test accuracy: 87.0 %\n",
            "Epoch: 132, Loss: 0.181, Test accuracy: 86.3 %\n",
            "Epoch: 133, Loss: 0.340, Test accuracy: 86.5 %\n",
            "Epoch: 134, Loss: 0.155, Test accuracy: 86.5 %\n",
            "Epoch: 135, Loss: 0.256, Test accuracy: 86.3 %\n",
            "Epoch: 136, Loss: 0.209, Test accuracy: 86.7 %\n",
            "Epoch: 137, Loss: 0.267, Test accuracy: 86.5 %\n",
            "Epoch: 138, Loss: 0.361, Test accuracy: 87.1 %\n",
            "Epoch: 139, Loss: 0.157, Test accuracy: 86.6 %\n",
            "Epoch: 140, Loss: 0.261, Test accuracy: 86.9 %\n",
            "Epoch: 141, Loss: 0.672, Test accuracy: 87.0 %\n",
            "Epoch: 142, Loss: 0.885, Test accuracy: 86.8 %\n",
            "Epoch: 143, Loss: 0.367, Test accuracy: 86.4 %\n",
            "Epoch: 144, Loss: 0.184, Test accuracy: 86.1 %\n",
            "Epoch: 145, Loss: 0.735, Test accuracy: 86.8 %\n",
            "Epoch: 146, Loss: 0.867, Test accuracy: 87.2 %\n",
            "Epoch: 147, Loss: 0.079, Test accuracy: 86.7 %\n",
            "Epoch: 148, Loss: 0.242, Test accuracy: 86.6 %\n",
            "Epoch: 149, Loss: 0.266, Test accuracy: 86.6 %\n",
            "Epoch: 150, Loss: 0.133, Test accuracy: 87.2 %\n",
            "Epoch: 151, Loss: 0.167, Test accuracy: 86.8 %\n",
            "Epoch: 152, Loss: 0.108, Test accuracy: 87.2 %\n",
            "Epoch: 153, Loss: 0.055, Test accuracy: 86.3 %\n",
            "Epoch: 154, Loss: 0.158, Test accuracy: 86.8 %\n",
            "Epoch: 155, Loss: 0.423, Test accuracy: 87.0 %\n",
            "Epoch: 156, Loss: 0.065, Test accuracy: 86.4 %\n",
            "Epoch: 157, Loss: 0.330, Test accuracy: 86.4 %\n",
            "Epoch: 158, Loss: 0.159, Test accuracy: 86.1 %\n",
            "Epoch: 159, Loss: 0.491, Test accuracy: 86.4 %\n",
            "Epoch: 160, Loss: 0.109, Test accuracy: 86.6 %\n",
            "Epoch: 161, Loss: 0.086, Test accuracy: 87.2 %\n",
            "Epoch: 162, Loss: 0.084, Test accuracy: 86.8 %\n",
            "Epoch: 163, Loss: 0.209, Test accuracy: 85.9 %\n",
            "Epoch: 164, Loss: 0.187, Test accuracy: 87.2 %\n",
            "Epoch: 165, Loss: 0.036, Test accuracy: 86.8 %\n",
            "Epoch: 166, Loss: 0.086, Test accuracy: 86.8 %\n",
            "Epoch: 167, Loss: 0.128, Test accuracy: 87.1 %\n",
            "Epoch: 168, Loss: 0.119, Test accuracy: 86.6 %\n",
            "Epoch: 169, Loss: 0.162, Test accuracy: 86.4 %\n",
            "Epoch: 170, Loss: 0.147, Test accuracy: 86.7 %\n",
            "Epoch: 171, Loss: 0.322, Test accuracy: 86.3 %\n",
            "Epoch: 172, Loss: 0.120, Test accuracy: 86.5 %\n",
            "Epoch: 173, Loss: 0.278, Test accuracy: 86.1 %\n",
            "Epoch: 174, Loss: 0.385, Test accuracy: 87.0 %\n",
            "Epoch: 175, Loss: 0.286, Test accuracy: 86.8 %\n",
            "Epoch: 176, Loss: 0.275, Test accuracy: 87.2 %\n",
            "Epoch: 177, Loss: 0.122, Test accuracy: 86.7 %\n",
            "Epoch: 178, Loss: 0.057, Test accuracy: 87.1 %\n",
            "Epoch: 179, Loss: 0.728, Test accuracy: 87.2 %\n",
            "Epoch: 180, Loss: 0.130, Test accuracy: 87.2 %\n",
            "Epoch: 181, Loss: 0.326, Test accuracy: 86.9 %\n",
            "Epoch: 182, Loss: 0.150, Test accuracy: 86.5 %\n",
            "Epoch: 183, Loss: 0.099, Test accuracy: 86.7 %\n",
            "Epoch: 184, Loss: 0.286, Test accuracy: 86.3 %\n",
            "Epoch: 185, Loss: 0.083, Test accuracy: 86.6 %\n",
            "Epoch: 186, Loss: 0.047, Test accuracy: 86.4 %\n",
            "Epoch: 187, Loss: 0.301, Test accuracy: 85.9 %\n",
            "Epoch: 188, Loss: 0.048, Test accuracy: 86.9 %\n",
            "Epoch: 189, Loss: 0.310, Test accuracy: 86.7 %\n",
            "Epoch: 190, Loss: 0.055, Test accuracy: 86.8 %\n",
            "Epoch: 191, Loss: 0.073, Test accuracy: 87.3 %\n",
            "Epoch: 192, Loss: 0.371, Test accuracy: 86.9 %\n",
            "Epoch: 193, Loss: 0.104, Test accuracy: 87.2 %\n",
            "Epoch: 194, Loss: 0.177, Test accuracy: 86.6 %\n",
            "Epoch: 195, Loss: 0.323, Test accuracy: 86.8 %\n",
            "Epoch: 196, Loss: 0.733, Test accuracy: 87.0 %\n",
            "Epoch: 197, Loss: 0.069, Test accuracy: 86.9 %\n",
            "Epoch: 198, Loss: 0.278, Test accuracy: 86.8 %\n",
            "Epoch: 199, Loss: 0.188, Test accuracy: 87.0 %\n",
            "Epoch: 200, Loss: 0.114, Test accuracy: 86.4 %\n",
            "Epoch: 201, Loss: 0.108, Test accuracy: 87.1 %\n",
            "Epoch: 202, Loss: 0.544, Test accuracy: 86.4 %\n",
            "Epoch: 203, Loss: 0.070, Test accuracy: 86.9 %\n",
            "Epoch: 204, Loss: 0.397, Test accuracy: 86.3 %\n",
            "Epoch: 205, Loss: 0.064, Test accuracy: 86.8 %\n",
            "Epoch: 206, Loss: 0.114, Test accuracy: 86.9 %\n",
            "Epoch: 207, Loss: 0.053, Test accuracy: 86.8 %\n",
            "Epoch: 208, Loss: 0.232, Test accuracy: 86.6 %\n",
            "Epoch: 209, Loss: 0.102, Test accuracy: 87.0 %\n",
            "Epoch: 210, Loss: 0.134, Test accuracy: 86.8 %\n",
            "Epoch: 211, Loss: 0.219, Test accuracy: 86.7 %\n",
            "Epoch: 212, Loss: 0.120, Test accuracy: 87.1 %\n",
            "Epoch: 213, Loss: 0.162, Test accuracy: 87.0 %\n",
            "Epoch: 214, Loss: 0.097, Test accuracy: 87.3 %\n",
            "Epoch: 215, Loss: 0.243, Test accuracy: 87.0 %\n",
            "Epoch: 216, Loss: 0.330, Test accuracy: 87.4 %\n",
            "Epoch: 217, Loss: 0.140, Test accuracy: 86.5 %\n",
            "Epoch: 218, Loss: 0.176, Test accuracy: 86.8 %\n",
            "Epoch: 219, Loss: 0.287, Test accuracy: 87.0 %\n",
            "Epoch: 220, Loss: 0.401, Test accuracy: 86.4 %\n",
            "Epoch: 221, Loss: 0.226, Test accuracy: 86.8 %\n",
            "Epoch: 222, Loss: 0.078, Test accuracy: 87.0 %\n",
            "Epoch: 223, Loss: 0.417, Test accuracy: 87.6 %\n",
            "Epoch: 224, Loss: 0.351, Test accuracy: 86.5 %\n",
            "Epoch: 225, Loss: 0.091, Test accuracy: 87.1 %\n",
            "Epoch: 226, Loss: 0.054, Test accuracy: 87.0 %\n",
            "Epoch: 227, Loss: 0.812, Test accuracy: 86.6 %\n",
            "Epoch: 228, Loss: 0.421, Test accuracy: 86.5 %\n",
            "Epoch: 229, Loss: 0.074, Test accuracy: 87.3 %\n",
            "Epoch: 230, Loss: 0.199, Test accuracy: 87.2 %\n",
            "Epoch: 231, Loss: 0.081, Test accuracy: 86.2 %\n",
            "Epoch: 232, Loss: 0.174, Test accuracy: 86.6 %\n",
            "Epoch: 233, Loss: 0.450, Test accuracy: 86.7 %\n",
            "Epoch: 234, Loss: 0.366, Test accuracy: 86.4 %\n",
            "Epoch: 235, Loss: 0.336, Test accuracy: 86.8 %\n",
            "Epoch: 236, Loss: 0.080, Test accuracy: 86.6 %\n",
            "Epoch: 237, Loss: 0.243, Test accuracy: 86.5 %\n",
            "Epoch: 238, Loss: 0.049, Test accuracy: 87.0 %\n",
            "Epoch: 239, Loss: 0.095, Test accuracy: 86.8 %\n",
            "Epoch: 240, Loss: 0.054, Test accuracy: 86.5 %\n",
            "Epoch: 241, Loss: 0.341, Test accuracy: 86.3 %\n",
            "Epoch: 242, Loss: 0.149, Test accuracy: 87.0 %\n",
            "Epoch: 243, Loss: 0.023, Test accuracy: 86.5 %\n",
            "Epoch: 244, Loss: 0.089, Test accuracy: 86.8 %\n",
            "Epoch: 245, Loss: 0.226, Test accuracy: 86.2 %\n",
            "Epoch: 246, Loss: 0.141, Test accuracy: 87.2 %\n",
            "Epoch: 247, Loss: 0.210, Test accuracy: 86.9 %\n",
            "Epoch: 248, Loss: 0.245, Test accuracy: 87.3 %\n",
            "Epoch: 249, Loss: 0.048, Test accuracy: 86.9 %\n",
            "Epoch: 250, Loss: 0.215, Test accuracy: 87.3 %\n",
            "Total time for training :  10608.193424224854\n"
          ]
        }
      ],
      "source": [
        "model_2 = PointNetFull()\n",
        "model_parameters_2 = filter(lambda p: p.requires_grad, model_2.parameters())\n",
        "print(\"Number of parameters in the Neural Networks: \", sum([np.prod(p.size()) for p in model_parameters_2]))\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device: \", device)\n",
        "model_2.to(device)\n",
        "train(model_2, device, train_loader, test_loader, epochs=250)\n",
        "print(\"Total time for training : \", time.time() - t0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Enregistrer Model**"
      ],
      "metadata": {
        "id": "gpqUG0pY97eQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZtTUZvCK6Yy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f1039d2e-967f-4446-c41b-4a6cccb46b8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to /content/model_PointNet_full_augm_2.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_85296194-0ce7-4fb3-a870-f25304100316\", \"model_PointNet_full_augm_2.pth\", 6552866)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Enregistrer le model\n",
        "save_path_f = '/content/model_PointNet_full_augm_2.pth'\n",
        "torch.save(model_2.state_dict(), save_path_f)\n",
        "print(f\"Model saved to {save_path_f}\")\n",
        "\n",
        "#telecharger le model\n",
        "from google.colab import files\n",
        "files.download(save_path_f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pifquKaGofGV"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}